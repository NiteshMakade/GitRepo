{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"C:/Users/User/Desktop/Eclerx/linkedin Learning/Processing_Text_with_python/Ex_Files_Text_Python_EssT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Exercise Files/code/Spark-Course-Description.txt\",\"rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
     ]
    }
   ],
   "source": [
    "corpus = PlaintextCorpusReader(os.getcwd(), \"Exercise Files/code/Spark-Course-Description.txt\")\n",
    "print(corpus.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent =corpus.sents()\n",
    "sent\n",
    "words = corpus.words()\n",
    "words\n",
    "paragraphs = corpus.paras()\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_freq_dist = nltk.FreqDist(corpus.words())\n",
    "course_freq_dist.most_common(10)\n",
    "course_freq_dist.get(\"to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "#cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = open(os.getcwd() + \"/Exercise Files/code/Spark-Course-Description.txt\",\"rt\")\n",
    "raw_text =base_file.read()\n",
    "base_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = nltk.word_tokenize(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', 'In', 'this', 'course', 'discover', 'how', 'to', 'build', 'big', 'data', 'pipelines', 'around', 'Apache', 'Spark', 'Join', 'Kumaran', 'Ponnambalam', 'as', 'he', 'takes', 'you', 'through', 'how', 'to', 'make', 'Apache', 'Spark', 'work', 'with', 'other', 'big', 'data', 'technologies', 'He', 'covers', 'the', 'basics', 'of', 'Apache', 'Kafka', 'Connect', 'and', 'how', 'to', 'integrate', 'it', 'with', 'Spark', 'for', 'real-time', 'streaming', 'In', 'addition', 'he', 'demonstrates', 'how', 'to', 'use', 'the', 'various', 'technologies', 'to', 'construct', 'an', 'end-to-end', 'project', 'that', 'solves', 'a', 'real-world', 'business', 'problem']\n"
     ]
    }
   ],
   "source": [
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "print(token_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list3 = [word.lower() for word in token_list2]\n",
    "#token_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
    "len(token_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming (Ex. combine/combining/combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list5 = [stemmer.stem(word) for word in token_list4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list6 = [Lemmatizer.lemmatize(word) for word in token_list4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technologies\n",
      "technolog\n",
      "technology\n"
     ]
    }
   ],
   "source": [
    "#check the result\n",
    "print(token_list4[20])\n",
    "print(token_list5[20])\n",
    "print(token_list6[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common bigrams: \n",
      "[(('big', 'data'), 3), (('data', 'pipeline'), 2), (('data', 'technology'), 2), (('apache', 'spark'), 2), (('order', 'construct'), 1)]\n"
     ]
    }
   ],
   "source": [
    "#find Bigrams and most common 5 \n",
    "bigrams = ngrams(token_list6, 2)\n",
    "print(\"most common bigrams: \" )\n",
    "print(Counter(bigrams).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('order', 'NN'),\n",
       " ('construct', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('pipelines', 'NNS'),\n",
       " ('networks', 'NNS'),\n",
       " ('stream', 'VBP'),\n",
       " ('process', 'NN'),\n",
       " ('store', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('data', 'NNS')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.pos_tag(token_list4)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['america', 'americatv', 'ball', 'basket', 'basketball', 'league', 'nba', 'popular', 'telecaste']\n",
      "(2, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.5       , 0.5       , 0.        ,\n",
       "        0.5       , 0.5       , 0.        , 0.        ],\n",
       "       [0.35355339, 0.35355339, 0.        , 0.        , 0.70710678,\n",
       "        0.        , 0.        , 0.35355339, 0.35355339]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#use small corpus for each visulization\n",
    "vector_corpus = [\n",
    "    'NBA is basket ball league',\n",
    "    'basketball is popular in america'\n",
    "    'TV in america telecaste basketball'\n",
    "    ]\n",
    "\n",
    "Vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "tfidf = Vectorizer.fit_transform(vector_corpus)\n",
    "\n",
    "print(Vectorizer.get_feature_names())\n",
    "\n",
    "print(tfidf.shape)\n",
    "\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
